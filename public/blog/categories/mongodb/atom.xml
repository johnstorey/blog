<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mongodb | John Dreams]]></title>
  <link href="http://blog.johnstorey.org/blog/categories/mongodb/atom.xml" rel="self"/>
  <link href="http://blog.johnstorey.org/"/>
  <updated>2014-07-31T21:30:05-07:00</updated>
  <id>http://blog.johnstorey.org/</id>
  <author>
    <name><![CDATA[John Storey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Node.js and mongoDB BOF]]></title>
    <link href="http://blog.johnstorey.org/blog/2011/12/09/node-dot-js-and-mongodb-bof/"/>
    <updated>2011-12-09T16:47:00-08:00</updated>
    <id>http://blog.johnstorey.org/blog/2011/12/09/node-dot-js-and-mongodb-bof</id>
    <content type="html"><![CDATA[<p><em>These are unedited notes from the MongoSV 2011 Conference</em></p>

<p>mongoDB node.js driver has native and JS BSON libraries. Many forums recommend using the JS one. Moderator agrees. He may split the JS driver out to a completely separate repository and project.</p>

<p>Now it does cool things like getting the raw, unparsed BSON data. Good for HTML5 games for example, where the parsing is too slow.</p>

<p>Then native C++ driver is 2X slower serializing data, and 5X faster deserializer. Moderator blames it on his own limited C++ skills. In practice he does not see this as a limiting factor in most applications.</p>

<h1>GridFS</h1>

<p>Moderator experimenting with a feature like GridFS. Uses a collection of documents that have a chunk in them that is binary. Wants to make binary types in mongo much more efficent. Basically the moment it hits binary, like a video, it starts streaming it without much overhead.</p>

<h1>Mongoose</h1>

<p>If you want performance, don&rsquo;t use it. It&rsquo;s all about validating types; getters and setters; more expressive query builder; etc. It&rsquo;s alot slower.</p>

<p>In V8 try / catch and getter / setter are just not optimized. Also there are some crazy overloading of operator types.</p>

<h1>Safe Writes</h1>

<p>Off by default in JS and most mongoDB drivers. Considering making it on by default. Otherwise there is no way for you to know if your transactions work for example.</p>

<h1>SSL Support</h1>

<p>v2.2 they will add SSL support to the drivers. It&rsquo;s only waiting on US Export license approval.</p>

<h1>Multi CPU systems</h1>

<p>In Node.js you used to have to fire up a node process per CPU. Now it&rsquo;s in a configuration file.</p>

<h1>Advantage of V8</h1>

<p>Because you are on V8 you get to use the latest JS stuff all the time. object.create(), not this.prototype for example.</p>

<p>Buffer objects are created outside of V8 heap and stays until you release it. But is direct memory access speed. Increased JS driver speed by 5x.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MoPub Scaling]]></title>
    <link href="http://blog.johnstorey.org/blog/2011/12/09/mopub-scaling/"/>
    <updated>2011-12-09T16:47:00-08:00</updated>
    <id>http://blog.johnstorey.org/blog/2011/12/09/mopub-scaling</id>
    <content type="html"><![CDATA[<p><em>These are unedited notes from the MongoSV 2011 Conference</em></p>

<p>50ms to process each event.</p>

<ul>
<li>Started as 3 founders hoping to bring Angry Birds on to the platform. Their customers are app developers.</li>
<li>Needed to scale quickly as customers came on</li>
<li>0 time to spend time on ops</li>
<li>chose Google App Engine</li>
<li>Python (?Guido? works on this team)</li>
</ul>


<p>Business objects are siloed per user account. One to many relationships.</p>

<p>Collect stats for every ad serve and ad interaction.</p>

<p>On app engine have a simple stats model.</p>

<h1>Realtime stats in GAE</h1>

<ul>
<li>datastore

<ul>
<li>expensive writes</li>
<li>slow writes 1 / sec/ entity group</li>
<li>fast reads 3 ms</li>
</ul>
</li>
<li>memcache</li>
<li>TaskQueues</li>
</ul>


<p>Cannot do sums over objects in GAE or even native map / reduces</p>

<p>Solution: Buffer into memcache then periodically flush to the datastore by pushing them into a TaskQueue.</p>

<h1>GAE Shortcomings</h1>

<ul>
<li>Transient downtime for writes</li>
<li>Read before write</li>
<li>memcache is fleeting, leading to data loss</li>
<li>Objects have become too large for transactions</li>
<li>writing to datastore, even buffered is expensive</li>
<li> pay for datastore CPU and app CPU. Way too costly.</li>
</ul>


<h1>AWS + Mongo</h1>

<ul>
<li>We have control of each service and the machines on which they run</li>
<li>access to RAM!</li>
<li>fire and forget increments</li>
<li>our entire machinery for recording mulitple stats is a series of increment instructions sent to MongoDB (which handles all buffering and flushing.)</li>
</ul>


<h1>First Attempt: Data Model</h1>

<p>One document per published advertiser per month. Each document stores data for all days and hours in the month.</p>

<h2>Initial results</h2>

<ul>
<li>Pros

<ul>
<li>Scales with # business objects</li>
<li>Queries for highel level cross products</li>
<li>Queries for stats for a given date range result in only reading the # of months total documents in the range</li>
</ul>
</li>
<li>Cons</li>
<li> Lots of little documents</li>
<li> more</li>
</ul>


<h1>Solution: New Data Model</h1>

<p>Account for ??? in our data model.</p>

<p>Everything in the account stored with hash table.</p>

<p>Created IndexDocument to store the reverse indices to do the higher-level cross sections</p>

<h2>Results</h2>

<ul>
<li>2 machines handle all the writes</li>
<li>Mongo load is now miniscule</li>
<li>each update really 2 (StatsDocument and StatsIndex)</li>
<li>Each get is really 2 gets (same objects)</li>
<li>One document per account per day</li>
<li> # documents scale with # accounts</li>
<li> we do the simple aggregation in python</li>
<li> size of document scales with # business objects within account</li>
<li> more</li>
</ul>


<h1>Lessons Learned</h1>

<ul>
<li>Better to have fewer, larger documents if there is a logical hierarchy than many smaller documents arranged flatly</li>
<li>Log everything because you will make mistakes</li>
<li> This has saved us time and again</li>
<li>Buy more memory</li>
<li> It&rsquo;s cheap!</li>
<li> Solved our cache misses</li>
<li> Some problems cannot be solved just by throwing memory at the problem</li>
</ul>


<h1>Future Work</h1>

<p>Mobile User store</p>

<ul>
<li>will record every interaction of every user</li>
<li>ML: map / reduce jobs to glean data about our suers and segment them</li>
<li>per user targeting with minimum latency</li>
<li>relevancy => better user experience => app devs make more money</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB on AWS]]></title>
    <link href="http://blog.johnstorey.org/blog/2011/12/09/mongodb-on-aws/"/>
    <updated>2011-12-09T16:46:00-08:00</updated>
    <id>http://blog.johnstorey.org/blog/2011/12/09/mongodb-on-aws</id>
    <content type="html"><![CDATA[<p><em>These are unedited notes from the MongoSV 2011 Conference</em></p>

<p>Jared Rosoff
<a href="&#109;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#x6a;&#x73;&#114;&#64;&#49;&#x30;&#x67;&#101;&#110;&#x2e;&#x63;&#111;&#109;">&#106;&#115;&#114;&#x40;&#x31;&#48;&#x67;&#101;&#110;&#x2e;&#x63;&#x6f;&#x6d;</a></p>

<h1>Single node setup</h1>

<h2>Instance sizing</h2>

<p>mongoDB&rsquo;s storage engine is meant to work on a 64-bit platform.</p>

<p>Micro instances are terrible DB servers. The CPU bursts then slows. You only want to do development and testing. You never want to run a production DB. But they make great arbiters and config servers.</p>

<p>Large + normal way people go. Cluster compute instances are great for really huge mongoDB instances.</p>

<p>mongoDB uses disk and RAM, so high CPU instance probably a waste of money.</p>

<h2>OS</h2>

<p>Any Gnu Linux good. 10gen starts with AMZN one.</p>

<ul>
<li>turn off atime</li>
<li><p>raise file descriptior limits</p>

<p>cat >> /etc/security/limits.conf &lt;&lt; EOF</p>

<ul>
<li>hard nofile 65536</li>
<li>soft profile 65536
EOF</li>
</ul>
</li>
<li><p>DO NOT use large VM pages</p></li>
<li>Use ext4, xfs (ext3 preallocate files and db will pause during this)</li>
<li>Use RAID</li>
<li> RAID10 on mongoD</li>
<li> RAID1 on configDB</li>
<li><em>Warning!</em> Known problems with Ubuntu 10.04 &amp; EBS</li>
</ul>


<h1>mongoDB Data Note</h1>

<p>Reccomend</p>

<ul>
<li>64 bit instance</li>
<li>more RAM = better</li>
<li>run ext4 or xfs file system</li>
<li>turn off atmie &amp; diratime</li>
<li>EBS volumes in RAID10</li>
</ul>


<p>MTBF of EBS > an actual disk drive. RAID10 helps compensate for this.</p>

<p>Each EBS can sustaine 100 IOPS (i/o per second) so more stripes the more IOPs per mongoDB instance. So more disk drives is very important. This run in production by many clients.</p>

<h2>Config Server</h2>

<ul>
<li>64 bit instance</li>
<li> micro is fine</li>
<li>EBS volumes in RAID1 is fine</li>
</ul>


<h2>Arbiter</h2>

<p>Just needs a network</p>

<ul>
<li>micro is fine</li>
<li>no storage requirements</li>
<li>must be separate from rest of replica sets</li>
</ul>


<h1>Single region replica set</h1>

<p>3 mongoDBs in a region, in separate availability zones. Gives some guarantee that the instances run on different boxes, avoiding a single point of failure.</p>

<p>Still vulnerable to a region going down.</p>

<h2>Disaster recovery site</h2>

<p>To avoid single region death people commonly keep 2 mongoDBs in region-1, and that is production. region-2 is just a hot copy. If region-1 goes down, we can stand up region-2 quickly and change the DNS entry. Maybe takes a few hours max until AWS sorts itself out.</p>

<h1>Multi Data Center</h1>

<p>Want to run in multiple regions with auto-failover. To do that you need 3 regions. Within mongoDB to get a quorum you need > 50% to select a master. So set up replica sets in 3 separate regions, region-1, region-2 and region-3.</p>

<p>When one region goes down the mongo replicate set will select a master from the other two.</p>

<p>You pay a price in latency. Writing may be across the country (assuming all regions are in the same country).</p>

<h1>Sharded Clusters</h1>

<p>definition: Lots of replica sets grouped together as a cluster.</p>

<p>So all the above advice applies. Just set up shards in each region instead of one instance.</p>

<h1>Provisioning</h1>

<p>Create one secruity group with mongodB instances. Create another with app servers. Grant access to mongoDB security group for app server group.</p>

<h1>Backups</h1>

<p>If you are using EBS you can take snapshots, even of multiple EBSs for RAID setups.</p>

<p>Other considerations are architecture specific.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Foursquare]]></title>
    <link href="http://blog.johnstorey.org/blog/2011/12/09/foursquare/"/>
    <updated>2011-12-09T16:46:00-08:00</updated>
    <id>http://blog.johnstorey.org/blog/2011/12/09/foursquare</id>
    <content type="html"><![CDATA[<p><em>These are unedited notes from the MongoSV 2011 Conference</em></p>

<h1>Stats</h1>

<ul>
<li>15 M users</li>
<li>Peak 2500 HTTP QPS</li>
<li>80 check ins per sec</li>
<li>8 production mongodb clisuter</li>
<li>8 shards of users, 12 shards of check-ins</li>
<li>Users: 250 updates / sec, 4k ops per sec, 46MB per sec</li>
</ul>


<p>Using Solr as their search stack.</p>

<p>EC2 supports 50 faults / sec / drive.</p>

<p>They retain the entire working set in RAM. Once they go to disk they see huge volume of errors.</p>

<p>Right now using 4 EBS volumes with RAID 0. Thinking about RAID 10.</p>

<p>Overtime fragmentation leads to bloat. If you view db.stats you can see this in data size, index size, and storage size.</p>

<h1>Problem: EBS performance degrades</h1>

<p>Solution? Kill the mongodb process and restart on a new EBS volume. Then do the same with the backup.</p>

<p>Symptons</p>

<ul>
<li>ioutil % on one volume > 90%</li>
<li>qr / qw counts spike</li>
<li>fault rates > 10 in mongostat</li>
<li>sometimes tcploss counts spike</li>
</ul>


<h1>Problem: Fresh Mongo instance has not paged in all the data</h1>

<p>Solution</p>

<ul>
<li>cat > /dev/null works too unless your dataset is > RAM</li>
</ul>


<p>Also cool ideas are in the Instagram blog with something they call &lsquo;intouch&rsquo;.</p>

<h1>Hits</h1>

<ul>
<li>Always set new replica set members to initialSync from a secondary or they will hammer primary</li>
<li>It&rsquo;s easy to inadvertently steal page cache from mongod (hint: less -n)</li>
<li>Booleans are 2 bytes long in BSON. We now use bitfields.</li>
</ul>


<h1>Backfills</h1>

<p>Common pattern: we want to instert / update a recond on every user / checkin / venue.</p>

<p>Used to do this sequentially by ID.</p>

<p>Now have a library that connects to mongoC, retrieves shard ranges, and runs 1 thread per shard.</p>

<h1>Migrating Collections</h1>

<p>You can migrate at the mongoDB level.</p>

<p>Oplogs to the rescue.</p>

<p>each oplog entry is idempotent, so long as data is replayed to the current op, consistency is.</p>

<p>This is basically how Mongo implements RECOVERING state.</p>

<h2>Not all Roses</h2>

<ul>
<li>Since moving from unshareded to sharded cluster, add to add shard key</li>
<li>Insert / update ops required transformation for shard key</li>
</ul>

]]></content>
  </entry>
  
</feed>
